# coding: utf-8
##########################################################################
# XXX - Copyright (C) XXX, 2017
# Distributed under the terms of the CeCILL-B license, as published by
# the CEA-CNRS-INRIA. Refer to the LICENSE file or to
# http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html
# for details.
#
##########################################################################

import numpy as np
import itertools
from joblib import Parallel, delayed
import pisap

import matplotlib.pyplot as plt
import scipy.fftpack as pfft


class ReportGridSearch():
    """A reporting class generated by the function grid_search containing all
       the reconstructed image, the given parameters, the error measurement,
       and differents methods to select the best parameters w.r.t a specific
       metrics.
    """
    def __init__(self, list_kwargs, recons_im, metrics_direction, errs):
        """ Init the class.

        Parameters:
        ----------
        list_kwargs: list of dictionary,
            the list of all the 'tested' parameter for the reconstruction.
        recons_im: list of pisap.base.image.Image
            the list of the reconstructed image. It should respect the same
            order than list_kwargs.
        metrics_direction: list of bool,
            specify if the metrics mean: True if the lower the better, False for
            the greater the better. It will be directly pass to the report
            result.
        errs: list of dictionary
            the list of all the metrics errors for each reconstructed image. It
            should respect the same order than list_kwargs (and so recons_im).
        """
        self.list_kwargs = list_kwargs
        self.recons_im = recons_im
        self.metrics_direction = metrics_direction
        self.errs = errs

    def all_score_(self, metric):
        """ Return all the score for the given metric.

        Parameters:
        -----------
        metric: string or function,
            the exact metric function name, example: 'compare_mse'
            from skimage.measure or the metric function itself.

        Return:
        ------
        all_score: np.ndarray,
            all the measure error for the given metric, respect the order same
            order than list_kwargs, recons_im and errs attributes.
        """
        if callable(metric):
            metric = metric.func_name
        return np.array([errs[metric] for errs in self.errs])

    def best_image_(self, metric):
        """ Return the best reconstructed image for the given metric.

        Parameters:
        -----------
        metric: string or function
            the exact metric function name, example: 'compare_mse'
            from skimage.measure.

        Return:
        -------
        best_image: pisap.base.image.Image,
            the best reconstructed image for the given metric.
        """
        best_idx = self.best_index_(metric)
        return self.recons_im[best_idx]

    def best_score_(self, metric):
        """ Return the best score for the given metric.

        Parameters:
        -----------
        metric: string or function
            the exact metric function name, example: 'compare_mse'
            from skimage.measure.

        Return:
        -------
        best_score: float,
            the best score for the given metric.
        """
        best_idx = self.best_index_(metric)
        return self.all_score_(metric)[best_idx]

    def best_params_(self, metric):
        """ Return the best set of parameters for the given metric.

        Parameters:
        -----------
        metric: string or function
            the exact metric function name, example: 'compare_mse'
            from skimage.measure.

        Return:
        -------
        best_params: dictionary,
            the best params for the given metric.
        """
        best_idx = self.best_index_(metric)
        return self.list_kwargs[best_idx]

    def best_index_(self, metric):
        """ Return the index of the best set of parameters for the given metric.

        Parameters:
        -----------
        metric: string or function
            the exact metric function name, example: 'compare_mse'
            from skimage.measure.

        Return:
        -------
        best_index: int,
            the index of the best set of parameters.
        """
        if callable(metric):
            metric = metric.func_name
        if self.metrics_direction[metric]:
            return np.argmin(self.all_score_(metric))
        else:
            return np.argmax(self.all_score_(metric))


def _wrap_im_metrics_func(metrics_funcs, im, ref):
    """ Helper to parallelize the metrics computations.
    """
    if callable(metrics_funcs):
        metrics_funcs = [metrics_funcs]
    err = {}
    for metrics_func in metrics_funcs:
        # im is a pisap.base.image.Image
        err[metrics_func.func_name] = metrics_func(im, ref)
    return err


def _wrap_recons_func(recons_func, **kwargs):
    """ Helper to parallelize the reconstruction.
    """
    res = recons_func(**kwargs)
    if isinstance(res, pisap.base.image.Image):
        return res
    elif isinstance(res, tuple):
        return res[0]
    else:
        raise ValueError(
            "res of 'recons_func' not understood: got {0}".format(type(res)))


def grid_search(recons_func, param_grid, metrics_funcs, metrics_direction,
                im_ref, n_jobs=1, verbose=0):
    """ Run `recons_func` on the carthesian product of `param_grid` then run each
        error measurement function in `metrics_funcs` w.r.t `im_ref`.

        Notes:
        -----
        `recons_func` should at least return an `pisap.base.image.Image` at
        first results (if multiple results).

        Parameters:
        -----------
        recons_func: function,
            the reconstruction function from whom to tune the hyperparameters.
            `recons_func` should at least return an `pisap.base.image.Image` at
            first results (if multiple results).
        param_grid : dict or list of dictionaries,
            Dictionary with parameters names (string) as keys and lists of
            parameter settings to try as values: the grids spanned by each
            dictionary in the list are explored.
        metrics_funcs: list of functions,
            The list of functions for the error measurement. Each one should
            only accept two arguments: im and ref and each function should
            return a real number.
        metrics_direction: list of bool,
            specify if the metrics mean: True if the lower the better, False for
            the greater the better. It will be directly pass to the report
            result.
        im_ref: np.ndarray
            This image reference for computing the error for each functions in
            metrics_funcs.
        n_jobs: int (default: 1),
            The maximum number of concurrently running jobs, such as the number
            of Python worker processes when backend=”multiprocessing” or the
            size of the thread-pool when backend=”threading”. If -1 all CPUs
            are used. If 1 is given, no parallel computing code is used at all,
            which is useful for debugging. For n_jobs below -1,
            (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2,
            all CPUs but one are used.
        verbose: int (default: 0),
            The verbosity level: if non zero, progress messages are printed.
            Above 50, the output is sent to stdout. The frequency of the
            messages increases with the verbosity level. If it more than 10,
            all iterations are reported. This argument crush the verbose
            argument in the recons_func function.

        Results:
        --------
        report: class,
            A reporting class containing all the reconstructed image, the given
            parameters, the error measurement, and differents methods to select
            the best parameters w.r.t a specific metrics.

        Exemples:
        --------
        >>> # imfft and im are declared previously...
        >>> list_params = {
        >>>    'data':imfft,
        >>>    'gradient_cls':Grad2DAnalyse,
        >>>    'gradient_kwargs':{"mask": mask_sh},
        >>>    'linear_cls':haarWaveletTransform,
        >>>    'linear_kwargs':[{"maxscale": 3}, {"maxscale": 4}],
        >>>    'max_nb_of_iter':50,
        >>>    'mu':[1.0e-6, 1.0e-4, 1.0e-2],
        >>>    'verbose':1, # note that this argument is crushed
        >>>                 # by the verbose argument of grid_search
        >>>    'report':False,
        >>> }
        >>> metrics = [compute_ssim, compute_snr, compute_psnr, compute_nrmse]
        >>> report = grid_search(sparse_rec_fista, list_params, metrics, im)
        >>> print(report.best_score_[compute_ssim])
        >>> print(report.best_score_[compute_snr])
    """
    # sanitize value to list type
    param_grid['verbose'] = verbose
    for key, value in param_grid.iteritems():
        if not isinstance(value, list):
            param_grid[key] = [value]
    list_kwargs = [dict(zip(param_grid, x))
                             for x in itertools.product(*param_grid.values())]
    if verbose > 0:
        print(("Running {0} metrics for {1} candidates, totalling {1} "
               "reconstruction and {2} metrics"
               " computations").format(len(metrics_funcs),
                                       len(list_kwargs),
                                       len(list_kwargs) * len(metrics_funcs),
                                        ))
    # Run the reconstruction
    recons_im = Parallel(n_jobs=n_jobs, verbose=verbose)(
                        delayed(_wrap_recons_func)(recons_func, **kwargs)
                                                    for kwargs in list_kwargs)
    # Compute the metrics
    errs = Parallel(n_jobs=n_jobs, verbose=verbose)(
                   delayed(_wrap_im_metrics_func)(metrics_funcs, im.data, im_ref)
                                                           for im in recons_im)
    metrics_direction = dict(zip([func.func_name for func in metrics_funcs],
                                 metrics_direction))
    return ReportGridSearch(list_kwargs, recons_im, metrics_direction, errs)
